---
title: "Active Learning Workshop"
author: "Bob Horton"
date: "November 21, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=FALSE, message=FALSE, fig.height=7.5)
rxOptions(reportProgress=0)
```

# Classifying wood knots

This is a followup to our earlier blog post "[Featurizing images: the shallow end of deep learning](http://blog.revolutionanalytics.com/2017/09/wood-knots.html#more)". That article contains the code for generating the features for the training and test datasets, which were saved to a csv file. Here we begin by loading that file.



```{r parameters}

### Libraries ###
library(dplyr)
library(tidyr)
library(ggplot2)

library(pROC)

### Meta-hyperparameters ###
set.seed(1)  ###

L1_PENALTY <- 1e-2
L2_PENALTY <- 1e-2

INITIAL_EXAMPLES_PER_CLASS <- 6  # cases from the labelled dataset used to train the initial model

BALANCE_CLASSES <- TRUE  # should we try to pick some cases from each of the three classes?

ADDITIONAL_CASES_TO_LABEL <- 2  # cases per class if BALANCE_CLASSES is TRUE

NUM_ITERATIONS <- 10

MONTE_CARLO_SAMPLES <- 100

# This order determines the order of factor levels
KNOT_CLASSES <- setNames(nm=c("sound_knot", "dry_knot", "encased_knot"))

LABELLED_FEATURIZED_DATA <- "labelled_knots_featurized_resnet18.Rds"
UNLABELLED_FEATURIZED_DATA <- "unlabelled_knots_featurized_large_resnet18.Rds"

unlabelled_knot_data_df <- readRDS(UNLABELLED_FEATURIZED_DATA)
labelled_knot_data_df <- readRDS(LABELLED_FEATURIZED_DATA)

PSEUDOLABELS_FILE <- "unlabelled_knot_info_area_small.csv" # "unlabelled_knot_info.csv" # We'll pretend these come from our labellers

inputs <- grep("^Feature", names(labelled_knot_data_df), value=TRUE)
outcome <- "knot_class"
FORM <- formula(paste(outcome, paste(inputs, collapse="+"), sep="~"))


```

### What does the data look like?

Here are the first five rows of the featurized labelled dataset:

```{r example_of_data}
labelled_knot_data_df[1:5, c(1, 2, 3, 4, 513, 514)]

```

The first column is the file name, the last column is the label, and the 512 numeric columns in between are the features computed by the deep neural network. We will be using these numbers for classifying the images, not the images themselves.

### Split labelled data into training and test sets

```{r split_train_and_test_sets}

class_samples <- lapply(KNOT_CLASSES, function(kc) sample(which(labelled_knot_data_df$knot_class == kc), INITIAL_EXAMPLES_PER_CLASS))

in_training_set <- (1:nrow(labelled_knot_data_df) %in% unlist(class_samples))

initial_training_set <- labelled_knot_data_df[in_training_set,]
TEST_SET <- labelled_knot_data_df[!in_training_set,]

table(initial_training_set$knot_class)


table(initial_training_set$knot_class)/nrow(initial_training_set)
```


## Initial model for knot classes

First we build a model on the available training data, and test on the test data. This model will focus on classifying a knot image into three categories: "sound_knot", "dry_knot", and "encased_knot". In the blog post there was also a category for "decayed_knot", but we will consider decay a separate attribute, and not use it in this model. We will use only a small number of the available labelled cases for training, and the rest for testing. This makes a larger test set and a smaller training set than in the blog post.

### Fit model to initial training set

```{r tune_and_train_model}
source("woodknots_active_learning_lib.R")

pseudolabel_function <- get_pseudolabelling_function(PSEUDOLABELS_FILE, KNOT_CLASSES)

initial_model_results <- fit_and_evaluate_model(initial_training_set)

```

## Results for initial model

#### Confusion matrix

```{r initial_model_confusion}

initial_model_results$confusion

```

### Performance summary

```{r initial_model_performance}
initial_model_results$performance

```

### Histograms of class scores

```{r class_score_histograms}

plot_class_histograms <- function(pred){
  pred %>% 
    gather(category, score, -c(path, knot_class, pred_class)) %>%
    ggplot(aes(x=score, col=knot_class)) + geom_histogram(bins=50) + facet_grid( knot_class ~ .)
}


plot_class_histograms(initial_model_results$test_predictions)

```

### ROC curves

```{r roc_curves}
# op <- par(mfrow=c(3,1), no.readonly = TRUE)
mapply(plot, x=initial_model_results$roc_list, main=names(initial_model_results$roc_list), print.auc=TRUE) %>% invisible
# par(op)
```

### Plot of test cases on entropy surface

Here we'll stick to a 2D representation, where the yellow background indicates regions of lower entropy.

```{r plot_initial_class_separation}


plot_class_separation(initial_model_results$test_predictions)

```

## Iterate modelling, case selection, and (pseudo) labelling

```{r iterate}

new_sample <- initial_model_results$selected %>% pseudolabel_function %>% get_new_pseudolabelled_sample

current_training_set <- rbind(initial_training_set, new_sample[names(initial_training_set)])

ALREADY_EVALUATED <- character(0)

iteration_results <- lapply(1:NUM_ITERATIONS, function(i){
  results <- fit_and_evaluate_model(current_training_set, been_there=ALREADY_EVALUATED)

  ALREADY_EVALUATED <<- c(ALREADY_EVALUATED, results$selected$path)
  results$selected_labelled <- results$selected %>% pseudolabel_function
  next_sample <- results$selected %>% pseudolabel_function %>% get_new_pseudolabelled_sample
  
  current_training_set <<- rbind(current_training_set, next_sample[names(current_training_set)])

  results
})
```


This shows the change in the metrics, with each row showing an iteration. The 'negentropy' metric is the negative entropy across all three class probabilities.

```{r visualize_metrics_by_iteration}
do.call("rbind", lapply(iteration_results, function(ires) ires$performance))

```

### Visualizing improvement for actively learned model

Here we plot a series of ROC curves showing how performance changes with iterations of active learning.

```{r visualizing_improvement}

plot_roc_history("sound", initial_model_results, iteration_results)
plot_roc_history("dry", initial_model_results, iteration_results)
plot_roc_history("encased", initial_model_results, iteration_results)
```



### Final model results

```{r final_model}
final_model_results <- iteration_results[[NUM_ITERATIONS]]
```

```{r final_model_confusion_matrix}
final_model_results$confusion
```

### Entropy surface plot

```{r classifier_evolution}
plot_class_separation(final_model_results$test_predictions, main="Final")
```

### Histograms of class scores for final model

```{r final_class_score_histograms}

plot_class_histograms(final_model_results$test_predictions)

```


Summary of performance using cases selected with active learning:

```{r summary_of_preformance_using_selected_cases}

(selected_sample_results <- final_model_results$performance)
```


## Monte Carlo Estimation of P-values

What is the probability that a set of randomly chosen cases would improve the performance of the model as much as the selected cases did? We'll add the same number of examples to the training set, except that now they will be randomly chosen. We'll repeat this sampling, training, and evaluation process `r MONTE_CARLO_SAMPLES` times, and see how many of those times we beat the performance of the selected cases.


```{r bootstrap_probability}

(N <- iteration_results[[NUM_ITERATIONS]]$tss - nrow(initial_training_set))

available_cases <- pseudolabel_function(unlabelled_knot_data_df)

random_sample_results <- sapply(1:MONTE_CARLO_SAMPLES, function(i){
  new_sample <- available_cases[sample(1:nrow(available_cases), N, replace=FALSE),]

  training_set_new <- rbind(initial_training_set, new_sample[names(initial_training_set)])

  fit_and_evaluate_model(training_set_new)$performance
})

```


```{r save_random_sample_results}
saveRDS(random_sample_results, sprintf("random_sample_results_iteration%02d.Rds", NUM_ITERATIONS))
```


### P-values

This table shows the number of times out of `r MONTE_CARLO_SAMPLES` tries that the randomly selected cases equalled or exceeded theperformance of the actively learned cases for each metric. These numbers are estimated P-values in percent.

```{r p_values}
mapply ( 
  function(metric) sum(random_sample_results[metric,] >= selected_sample_results[[metric]]), 
  row.names(random_sample_results)
) / MONTE_CARLO_SAMPLES


```

## Model trained with all available "unlabelled" cases

For comparison, we'll build a model as though we had gone through and labelled all `r nrow(available_cases)` of the useable new examples.

```{r full_model_results}
training_set_full <- rbind(initial_training_set, available_cases[names(initial_training_set)])

full_model_results <- fit_and_evaluate_model(training_set_full)

full_model_results$confusion

full_model_results$performance

plot_class_separation(full_model_results$test_predictions)

```
```{r save_training_set_full}
# saveRDS(training_set_full, "training_set_full.Rds")
# saveRDS(test_set, "test_set.Rds")
```

