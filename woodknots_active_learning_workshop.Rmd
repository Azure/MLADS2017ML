---
title: "Active Learning Workshop"
author: "Bob Horton"
date: "November 21, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=FALSE, message=FALSE)
rxOptions(reportProgress=0)
```

# Classifying wood knots

This is a followup to our earlier blog post "[Featurizing images: the shallow end of deep learning](http://blog.revolutionanalytics.com/2017/09/wood-knots.html#more)". That article contains the code for generating the features for the training and test datasets, which were saved to a csv file. Here we begin by loading that file.



```{r collect_unlabelled_image_data}

### Libraries ###
library(dplyr)
library(pROC)

### Meta-hyperparameters ###
set.seed(1)  ###

L1_PENALTY <- 1e-2
L2_PENALTY <- 1e-2

INITIAL_EXAMPLES_PER_CLASS <- 6  # cases from the labelled dataset used to train the initial model

BALANCE_CLASSES <- TRUE  # should we try to pick some cases from each of the three classes?

ADDITIONAL_CASES_TO_LABEL <- 2  # cases per class if BALANCE_CLASSES is TRUE

NUM_ITERATIONS <- 10

NUM_BOOTSTRAPS <- 100

# This order determines the order of factor levels
KNOT_CLASSES <- setNames(nm=c("sound_knot", "dry_knot", "encased_knot"))

LABELLED_FEATURIZED_DATA <- "labelled_knots_featurized_resnet18.Rds"
UNLABELLED_FEATURIZED_DATA <- "unlabelled_knots_featurized_resnet18.Rds"

PSEUDOLABELS_FILE <- "unlabelled_knot_info.csv" # We'll pretend these come from our labellers

unlabelled_knot_data_df <- readRDS(UNLABELLED_FEATURIZED_DATA)
labelled_knot_data_df <- readRDS(LABELLED_FEATURIZED_DATA)

inputs <- grep("^Feature", names(labelled_knot_data_df), value=TRUE)
outcome <- "knot_class"
FORM <- formula(paste(outcome, paste(inputs, collapse="+"), sep="~"))


```

### What does the data look like?

Here are the first five rows of the featurized labelled dataset:

```{r example_of_data}
labelled_knot_data_df[1:5, c(1,2, 3, 4, 513, 514)]

```

The first column is the file name, the last column is the label, and the 512 numeric columns in between are the features computed by the deep neural network. We will be using these numbers for classifying the images, not the images themselves.


```{r image_of_feature_set, eval=FALSE, echo=FALSE}
# Here is an image of all of these features for all the images:

M <- as.matrix(labelled_knot_data_df[2:513])
image(1:ncol(M), 1:nrow(M), t(M[nrow(M):1,]))
lines(5*colMeans(M))  # show the (exaggerated) mean for each column of pixels

```

### Knot class model

First we build a model on the available training data, and test on the test data. This model will focus on classifying a knot image into three categories: "sound_knot", "dry_knot", and "encased_knot". In the blog post there was also a category for "decayed_knot", but we will consider decay a separate attribute, and not use it in this model. We will use only a small number of the available labelled cases for training, and the rest for testing. This makes a larger test set and a smaller training set than in the blog post.


```{r split_train_and_test_sets}

# knot_data_df <- labelled_knot_data_df


class_samples <- lapply(KNOT_CLASSES, function(kc) sample(which(labelled_knot_data_df$knot_class == kc), INITIAL_EXAMPLES_PER_CLASS))

in_training_set <- (1:nrow(labelled_knot_data_df) %in% unlist(class_samples))

initial_training_set <- labelled_knot_data_df[in_training_set,]
TEST_SET <- labelled_knot_data_df[!in_training_set,]

table(initial_training_set$knot_class)

```


```{r tune_and_train_model}
source("woodknots_active_learning_lib.R")

pseudolabel_function <- get_pseudolabelling_function(PSEUDOLABELS_FILE, KNOT_CLASSES)

initial_model_results <- fit_and_evaluate_model(initial_training_set)

```
## Results for initial model

### Confusion matrix

```{r initial_model_confuaion}

initial_model_results$confusion

# Most of them are sound knots, with very few encased knots
rowSums(initial_model_results$confusion)/sum(initial_model_results$confusion)

```

### Performance summary

```{r initial_model_performance}
initial_model_results$performance

```

### Histograms of class scores

```{r class_score_histograms}

with(initial_model_results$test_predictions, {
  hist(sound_knot, breaks=100)
  hist(dry_knot, breaks=100)
  hist(encased_knot, breaks=100)
})

```

### ROC curves

```{r roc_curves}
res <- mapply(plot, x=initial_model_results$roc_list, main=names(initial_model_results$roc_list), print.auc=TRUE)

```

### Plot of test cases on entropy surface

Here is a 3d entropy surface you can try at home:

```{r simulated_entropy data, eval=FALSE}
library(rgl)
data.frame(x=runif(1e4), y=runif(1e4)) %>% 
  filter(x+y<1) %>% 
  mutate(z=entropy(x, y)) %>%
  with(plot3d(x,y,z))
```

Here we'll stick to a 2D representation, where the yellow background indicates regions of lower entropy.

```{r plot_initial_class_separation, fig.height=7}


plot_class_separation(initial_model_results$test_predictions)

```

## Iterate modelling, case selection, and (pseudo) labelling

```{r iterate}

get_new_pseudolabelled_sample <- function(labelled_filenames){
  labelled_filenames <- labelled_filenames[!is.na(labelled_filenames$path),]
  row.names(labelled_filenames) <- labelled_filenames$path
  pls <- unlabelled_knot_data_df[unlabelled_knot_data_df$path %in% labelled_filenames$path, ]
  pls$knot_class <- labelled_filenames[pls$path, "knot_class"]
  pls
}

new_sample <- initial_model_results$selected %>% pseudolabel_function %>% get_new_pseudolabelled_sample

current_training_set <- rbind(initial_training_set, new_sample[names(initial_training_set)])

ALREADY_EVALUATED <- character(0)

iteration_results <- lapply(1:NUM_ITERATIONS, function(i){
  results <- fit_and_evaluate_model(current_training_set, been_there=ALREADY_EVALUATED)

  ALREADY_EVALUATED <<- c(ALREADY_EVALUATED, results$selected$path)
  results$selected_labelled <- results$selected %>% pseudolabel_function
  next_sample <- results$selected %>% pseudolabel_function %>% get_new_pseudolabelled_sample
  
  current_training_set <<- rbind(current_training_set, next_sample[names(current_training_set)])

  results
})
```


These are the cases selected at each iteration:
```{r info_for_selected_cases}
lapply(iteration_results, function(ires) ires$selected_labelled)
```

This shows the change in the metrics, with each row showing an iteration. The 'negentropy' metric is the negative entropy across all three class probabilities.

```{r visualize_metrics_by_iteration}
do.call("rbind", lapply(iteration_results, function(ires) ires$performance))

```

### Visualizing improvement for actively learned model

Here we plot a series of ROC curves showing how performance changes with iterations of active learning.

```{r visualizing_improvement}

plot_roc_history("sound", initial_model_results, iteration_results)
plot_roc_history("dry", initial_model_results, iteration_results)
plot_roc_history("encased", initial_model_results, iteration_results)
```

### Classifier evolution

```{r classifier_evolution, fig.height=7}
for (i in seq_along(iteration_results)){
    plot_class_separation(iteration_results[[i]]$test_predictions, 
                          previous=if(1==i){
                                      initial_model_results$test_predictions 
                                    } else { 
                                      iteration_results[[i-1]]$test_predictions
                                    },
                          main=sprintf("Iteration %d", i)
    )
}
plot_class_separation(iteration_results[[length(iteration_results)]]$test_predictions, main="Final")
```


### Final model results

```{r final_model}
final_model_results <- iteration_results[[NUM_ITERATIONS]]

final_model_results$confusion
```


```{r model_coefficients, eval=FALSE, echo=FALSE}
### Model Coefficients

final_coef <- final_model_results$model %>% coef

final_coef %>% length

final_coef %>% plot(pch='.')

final_coef %>% names %>% gsub("[^\\d]*([\\d]+)", "\\1", ., perl=TRUE) %>% table %>% sort %>% rev %>% head

# saveRDS(final_model_results, "final_model_results.Rds")

```

### Histograms of class scores for final model

```{r final_class_score_histograms}

with(final_model_results$test_predictions, {
  hist(sound_knot, breaks=100)
  hist(dry_knot, breaks=100)
  hist(encased_knot, breaks=100)
})
```



## Bootstrapping P-values

What is the probability that a set of randomly chosen cases would improve the performance of the model as much as the selected cases did? We'll add the same number of examples to the training set, except that now they will be randomly chosen. We'll repeat this sampling, training, and evaluation process `r NUM_BOOTSTRAPS` times, and see how many of those times we beat the performance of the selected cases.


```{r bootstrap_probability}

(N <- iteration_results[[NUM_ITERATIONS]]$tss - nrow(initial_training_set))

available_cases <- pseudolabel_function(unlabelled_knot_data_df)

random_sample_results <- sapply(1:NUM_BOOTSTRAPS, function(i){
  new_sample <- available_cases[sample(1:nrow(available_cases), N, replace=FALSE),]

  training_set_new <- rbind(initial_training_set, new_sample[names(initial_training_set)])

  fit_and_evaluate_model(training_set_new)$performance
})

```


```{r save_random_sample_results}
saveRDS(random_sample_results, sprintf("random_sample_results_iteration%02d.Rds", NUM_ITERATIONS))
```

Summary of random sample performance:

```{r random_sample_results}

random_sample_results[,1:4]

apply(random_sample_results, 1, mean)
apply(random_sample_results, 1, sd)
apply(random_sample_results, 1, max)

```

Summary of performance using cases selected with active learning:

```{r summary_of_preformance_using_selected_cases}

(selected_sample_results <- iteration_results[[length(iteration_results)]]$performance)
```

### P-values

This table shows the number of times out of `r NUM_BOOTSTRAPS` tries that the randomly selected cases equalled or exceeded theperformance of the actively learned cases for each metric. These numbers are estimated P-values in percent.

```{r p_values}
mapply ( 
  function(metric) sum(random_sample_results[metric,] >= selected_sample_results[[metric]]), 
  row.names(random_sample_results)
) / NUM_BOOTSTRAPS

# combine AUCs
sum(colMeans(random_sample_results[2:4,]) >= mean(unlist(selected_sample_results[2:4]))) / NUM_BOOTSTRAPS


```

## Model trained with all available "unlabelled" cases

For comparison, we'll build a model as though we had gone through and labelled all `r nrow(available_cases)` of the useable new examples.

```{r full_model_results, fig.height=7.5}
training_set_full <- rbind(initial_training_set, available_cases[names(initial_training_set)])

full_model_results <- fit_and_evaluate_model(training_set_full)

full_model_results$confusion

full_model_results$performance

plot_class_separation(full_model_results$test_predictions)

```
```{r save_training_set_full}
# saveRDS(training_set_full, "training_set_full.Rds")
# saveRDS(test_set, "test_set.Rds")
```

